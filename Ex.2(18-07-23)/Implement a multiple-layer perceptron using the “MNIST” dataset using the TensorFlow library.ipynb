{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3043a952",
   "metadata": {},
   "source": [
    "## Implement a multiple-layer perceptron using the “MNIST” dataset using the TensorFlow library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ec778",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8258d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense,Conv2D,Input,Flatten,Dropout,MaxPool2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.utils import to_categorical,np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a37de6",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "594ab0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist=tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f7cfad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test)=mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e42af",
   "metadata": {},
   "source": [
    "#### Printing the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d68e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047b347",
   "metadata": {},
   "source": [
    "#### Astype conversion & normalizing the image pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96f221d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6c750",
   "metadata": {},
   "source": [
    "#### Flattening the input image(only integer scalar arrays can be converted to a scalar index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93cc2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09dca1",
   "metadata": {},
   "source": [
    "#### One-hot encoding of categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeb3e44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape before one-hot encoding: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b24839f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdadbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=10\n",
    "# 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c65d50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np_utils.to_categorical(y_train,n_classes)\n",
    "y_test=np_utils.to_categorical(y_test,n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1556c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after one-hot encoding:  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape after one-hot encoding: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0dd35",
   "metadata": {},
   "source": [
    "#### Building a linear stack of layers with the sequential model (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aba24a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74510500",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(100,input_shape=(784,),activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "459134d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "335dee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90d2d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.3800 - accuracy: 0.8958 - val_loss: 0.2189 - val_accuracy: 0.9359\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1852 - accuracy: 0.9469 - val_loss: 0.1603 - val_accuracy: 0.9535\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1361 - accuracy: 0.9612 - val_loss: 0.1257 - val_accuracy: 0.9632\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1075 - accuracy: 0.9693 - val_loss: 0.1098 - val_accuracy: 0.9674\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.0877 - accuracy: 0.9747 - val_loss: 0.0976 - val_accuracy: 0.9706\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0740 - accuracy: 0.9787 - val_loss: 0.0939 - val_accuracy: 0.9724\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0619 - accuracy: 0.9822 - val_loss: 0.0829 - val_accuracy: 0.9759\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0531 - accuracy: 0.9850 - val_loss: 0.0823 - val_accuracy: 0.9755\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0458 - accuracy: 0.9872 - val_loss: 0.0806 - val_accuracy: 0.9753\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0393 - accuracy: 0.9886 - val_loss: 0.0716 - val_accuracy: 0.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c81331f70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model for 10 epochs\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score=model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbe38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Score:\",score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test accuracy: \",score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8620d75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne major advantage of using ConvNets over NNs is that you do not need \\nto flatten the input images to 1D as they are capable of working with \\nimage data in 2D. \\nThis helps in retaining the “spatial” properties of images.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "One major advantage of using ConvNets over NNs is that you do not need \n",
    "to flatten the input images to 1D as they are capable of working with \n",
    "image data in 2D. \n",
    "This helps in retaining the “spatial” properties of images.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bda889",
   "metadata": {},
   "source": [
    "#### Building a linear stack of layers with the sequential model (CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee6820d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2= Sequential()\n",
    "model2.add(Conv2D(25,kernel_size=(3,3),strides=(1,1),padding='valid',input_shape=(28,28,2)))\n",
    "model2.add(MaxPool2D(pool_size=(1,1)))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(100,activation='relu'))\n",
    "model2.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1c98fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97426020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0345 - accuracy: 0.9905 - val_loss: 0.0729 - val_accuracy: 0.9780\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0293 - accuracy: 0.9925 - val_loss: 0.0766 - val_accuracy: 0.9778\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0264 - accuracy: 0.9927 - val_loss: 0.0715 - val_accuracy: 0.9786\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0219 - accuracy: 0.9946 - val_loss: 0.0749 - val_accuracy: 0.9777\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0187 - accuracy: 0.9955 - val_loss: 0.0757 - val_accuracy: 0.9780\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.0165 - accuracy: 0.9962 - val_loss: 0.0750 - val_accuracy: 0.9781\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0149 - accuracy: 0.9967 - val_loss: 0.0759 - val_accuracy: 0.9783\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0121 - accuracy: 0.9976 - val_loss: 0.0780 - val_accuracy: 0.9795\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0110 - accuracy: 0.9979 - val_loss: 0.0771 - val_accuracy: 0.9782\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0094 - accuracy: 0.9984 - val_loss: 0.0803 - val_accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c83758b50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=128,epochs=10,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743ffa3",
   "metadata": {},
   "source": [
    "##### Flatten the input image dimensions to 1D (width pixels x height pixels)\n",
    "##### Normalize the image pixel values (divide by 255)\n",
    "##### One-Hot Encode the categorical column\n",
    "##### Build a model architecture (Sequential) with Dense layers(Fully connected layers)\n",
    "##### Train the model and make predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
